import _pyscript  # get a core from PyScript module (pyscript.core)

# tokens offset
DOUBLE = _pyscript.constants.DOUBLE
TRIPLE = _pyscript.constants.TRIPLE
WITH_EQ = _pyscript.constants.WITH_EQ
SPECIAL = _pyscript.constants.SPECIAL

# constants
TOKENS = _pyscript.constants.TOKENS.copy()
KEYWORDS = _pyscript.constants.KEYWORDS.copy()

# flags
DEFAULT = _pyscript.constants.DEFAULT
COMMENT = _pyscript.constants.COMMENT
REVERSE_POW_XOR = _pyscript.constants.REVERSE_POW_XOR

# highlight formatter
HLFMT_HTML = _pyscript.highlight.HLFMT_HTML
HLFMT_ANSI = _pyscript.highlight.HLFMT_ANSI

class Parser {

    func __init__(self, source, flags=DEFAULT) {

        # source is a list of tokens
        if isinstance(source, list) {
            if not source
                # if the source list is empty (if it's empty there should be at least a PysToken(EOF))
                throw ValueError("empty list")

            if not all(comprehension(source, func(token) return isinstance(token, _pyscript.token.PysToken)))
                # if the list order one of which isn't a PysToken object
                throw TypeError("invalid list of token")

            self._compiled = 'tokenize'
            self.source = source
            self.file = source[0].position.file
        }

        # source is an AST
        elif isinstance(source, _pyscript.nodes.PysNode) {
            self._compiled = 'ast'
            self.source = source
            self.file = source.position.file
        }

        # source is an object that is supported and converted to a buffer
        else {
            self._compiled = 'none'
            self.source = source
            self.file = _pyscript.buffer.PysFileBuffer(source)
        }

        self.flags = flags
    }

    func tokenize(self) {
        if self._compiled == 'tokenize'
            return self.source
        elif self._compiled == 'ast'
            throw TypeError("tokenize(): can't tokenize AST source")

        lexer = _pyscript.lexer.PysLexer(
            file=self.file,
            flags=self.flags
        )

        [tokens, error] = lexer.make_tokens()
        if error
            throw error.exception

        return tokens
    }

    func ast(self, mode='exec') {
        if mode not in {'exec', 'eval'}
            throw ValueError("ast(): mode must be 'exec' or 'eval'")

        if self._compiled == 'tokenize'
            tokens = self.source
        elif self._compiled == 'ast'
            return self.source
        else
            tokens = self.tokenize()

        parser = _pyscript.parser.PysParser(
            file=self.file,
            tokens=tokens,
            flags=self.flags
        )

        ast = parser.parse(mode == 'exec' ? None : parser.expr)
        if ast.error
            throw ast.error.exception

        return ast.node
    }

    func analyze(self, mode='exec') {
        if self._compiled == 'ast'
            ast = self.source
        else
            ast = self.ast(mode=mode)

        analyzer = _pyscript.analyzer.PysAnalyzer(
            file=self.file,
            flags=self.flags
        )

        error = analyzer.analyze(ast)
        if error
            throw error.exception
    }

    func highlight(self, format=None, max_parenthesis_level=3) {
        # this function only supports direct source buffers
        return _pyscript.highlight.pys_highlight(
            source=self.file,
            format=format,
            max_parenthesis_level=max_parenthesis_level,
            flags=self.flags
        )
    }

}

# wrap functions

func tokenize(source, flags=DEFAULT)
    return Parser(source, flags).tokenize()

func ast(source, mode='exec', flags=DEFAULT)
    return Parser(source, flags).ast(mode=mode)

func analyze(source, mode='exec', flags=DEFAULT)
    return Parser(source, flags).analyze(mode=mode)

func highlight(source, format=None, max_parenthesis_level=3, flags=DEFAULT)
    return Parser(source, flags).highlight(format=format, max_parenthesis_level=max_parenthesis_level)